{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8f393e",
   "metadata": {},
   "source": [
    "Exercise 1: Answer the quiz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1967dd",
   "metadata": {},
   "source": [
    "Exercise-2\n",
    "\n",
    "Install NLTK library and its dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113c1a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing NLTK library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a41619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK dependences\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3cd7f0",
   "metadata": {},
   "source": [
    "Exercise -3\n",
    "\n",
    "Install scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f2f04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sushm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096dc0a",
   "metadata": {},
   "source": [
    "Exercise -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b048999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "This is a simple example of text preprocessing. it involves cleaning and organizing new text data into a format suitable for analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sushm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sushm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sushm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "#%% Sample text\n",
    "text = \"This is a simple example of text preprocessing. it involves cleaning and organizing new text data into a format suitable for analysis\" \n",
    "\n",
    "print (\"Original Text:\")\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ebcd7",
   "metadata": {},
   "source": [
    "Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145e1802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower case Text: \n",
      "this is a simple example of text preprocessing. it involves cleaning and organizing new text data into a format suitable for analysis\n"
     ]
    }
   ],
   "source": [
    "# Covert to lowercase\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "print(\"lower case Text: \")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c117a5",
   "metadata": {},
   "source": [
    "Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f54dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed punctuation : \n",
      "this is a simple example of text preprocessing it involves cleaning and organizing new text data into a format suitable for analysis\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "text= text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(\"Removed punctuation : \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d31589",
   "metadata": {},
   "source": [
    "Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29553d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens:\n",
      "['this', 'is', 'a', 'simple', 'example', 'of', 'text', 'preprocessing', 'it', 'involves', 'cleaning', 'and', 'organizing', 'new', 'text', 'data', 'into', 'a', 'format', 'suitable', 'for', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens =word_tokenize(text)\n",
    "print(\"All tokens:\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791a0a5",
   "metadata": {},
   "source": [
    "Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9425acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Text Preprocessing: \n",
      "['simple', 'example', 'text', 'preprocessing', 'involves', 'cleaning', 'organizing', 'new', 'text', 'data', 'format', 'suitable', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(\"\\nAfter Text Preprocessing: \")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab22a7",
   "metadata": {},
   "source": [
    "Exercise -9\n",
    "\n",
    "Now you have complete pre-processing steps try 3 different sentectses and repeat exercise 5-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b2fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: \n",
      "Artifical intelligence is changing the world\n",
      "\n",
      "\n",
      "Original Text: \n",
      "Data science uses algorithms to extract knowledge from data.\n",
      "\n",
      "\n",
      "Original Text: \n",
      "Robots can help humans by doing repetitive tasks faster.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Artifical intelligence is changing the world\"\n",
    "sentence2 = \"Data science uses algorithms to extract knowledge from data.\"\n",
    "sentence3 = \"Robots can help humans by doing repetitive tasks faster.\"\n",
    "# 3 different text\n",
    "\n",
    "sentences = [sentence1, sentence2, sentence3]\n",
    "for text in sentences:\n",
    "    print(\"\\nOriginal Text: \")\n",
    "    print(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d948effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower case Text: \n",
      "Artifical intelligence is changing the world\n",
      "\n",
      "lower case Text: \n",
      "Data science uses algorithms to extract knowledge from data.\n",
      "\n",
      "lower case Text: \n",
      "Robots can help humans by doing repetitive tasks faster.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coverting to lowecase\n",
    "text = text.lower()\n",
    "for text in sentences:\n",
    "    print(\"lower case Text: \")\n",
    "    print(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82efee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed punctuation:\n",
      "Artifical intelligence is changing the world\n",
      "\n",
      "Removed punctuation:\n",
      "Data science uses algorithms to extract knowledge from data\n",
      "\n",
      "Removed punctuation:\n",
      "Robots can help humans by doing repetitive tasks faster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "no_punc_texts = [text.translate(str.maketrans('', '', string.punctuation)) for text in sentences]\n",
    "\n",
    "\n",
    "for text in no_punc_texts:\n",
    "    print(\"Removed punctuation:\")\n",
    "    print(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b88a1928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens:\n",
      "['Artifical', 'intelligence', 'is', 'changing', 'the', 'world'] \n",
      "\n",
      "All tokens:\n",
      "['Data', 'science', 'uses', 'algorithms', 'to', 'extract', 'knowledge', 'from', 'data'] \n",
      "\n",
      "All tokens:\n",
      "['Robots', 'can', 'help', 'humans', 'by', 'doing', 'repetitive', 'tasks', 'faster'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "token_lists = [word_tokenize(text) for text in no_punc_texts]\n",
    "for tokens in token_lists:\n",
    "    print(\"All tokens:\")\n",
    "    print(tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad3525d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Text Preprocessing:\n",
      "['Artifical', 'intelligence', 'changing', 'world'] \n",
      "\n",
      "After Text Preprocessing:\n",
      "['Data', 'science', 'uses', 'algorithms', 'extract', 'knowledge', 'data'] \n",
      "\n",
      "After Text Preprocessing:\n",
      "['Robots', 'help', 'humans', 'repetitive', 'tasks', 'faster'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "filtered_tokens_list = [[word for word in tokens if word not in stop_words] for tokens in token_lists]\n",
    "for filtered_tokens in filtered_tokens_list:\n",
    "    print(\"After Text Preprocessing:\")\n",
    "    print(filtered_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939d394",
   "metadata": {},
   "source": [
    "Exercise 10: \n",
    "\n",
    " Create a text file using a notepad with a paragraph about any topic you prefer \n",
    "then read a text file using python. (Tip: to open a file in python you can use the same method \n",
    "as in week 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05c156ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Content:\n",
      " Artificial intelligence is transforming the world. \n",
      "It is used in healthcare, finance, education, and many other fields. \n",
      "AI technologies are improving efficiency and decision-making.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('text.txt', 'r') as file:\n",
    "    file_text = file.read()\n",
    "\n",
    "print(\"File Content:\\n\", file_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c669bd",
   "metadata": {},
   "source": [
    "Exercise 11 :  \n",
    "\n",
    "Apply the preprocessing step on the text you read from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24c90a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text\n",
      "Artificial intelligence is transforming the world. \n",
      "It is used in healthcare, finance, education, and many other fields. \n",
      "AI technologies are improving efficiency and decision-making.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the text file\n",
    "\n",
    "file =  open(\"text.txt\")\n",
    "file_text =  file.read()\n",
    "file.close()\n",
    "\n",
    "print(\"Original Text\")\n",
    "print(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebd5d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower case Text: \n",
      "artificial intelligence is transforming the world. \n",
      "it is used in healthcare, finance, education, and many other fields. \n",
      "ai technologies are improving efficiency and decision-making.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coversion to lowercase\n",
    "\n",
    "file_txt = file_text.lower()\n",
    "print(\"lower case Text: \")\n",
    "\n",
    "print(file_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6178065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove Punctuation\n",
      "Artificial intelligence is transforming the world \n",
      "It is used in healthcare finance education and many other fields \n",
      "AI technologies are improving efficiency and decisionmaking\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  remove punctuation\n",
    "\n",
    "file_txt = file_text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Remove Punctuation\")\n",
    "print(file_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b085e645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens\n",
      "['Artificial', 'intelligence', 'is', 'transforming', 'the', 'world', '.', 'It', 'is', 'used', 'in', 'healthcare', ',', 'finance', ',', 'education', ',', 'and', 'many', 'other', 'fields', '.', 'AI', 'technologies', 'are', 'improving', 'efficiency', 'and', 'decision-making', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(file_text)\n",
    "print(\"All tokens\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba10fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Text Preprocessing\n",
      "['Artificial', 'intelligence', 'transforming', 'world', '.', 'It', 'used', 'healthcare', ',', 'finance', ',', 'education', ',', 'many', 'fields', '.', 'AI', 'technologies', 'improving', 'efficiency', 'decision-making', '.']\n"
     ]
    }
   ],
   "source": [
    "#  Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens =[word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(\"\\nAfter Text Preprocessing\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ae040",
   "metadata": {},
   "source": [
    "Exercise 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c37489e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: cats, Lemma: cat\n",
      "Word: cacti, Lemma: cactus\n",
      "Word: geese, Lemma: goose\n",
      "Word: rocks, Lemma: rock\n",
      "Word: python, Lemma: python\n",
      "Word: better, Lemma: better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer =  WordNetLemmatizer()\n",
    "\n",
    "words = ['cats', 'cacti', 'geese', 'rocks', 'python', 'better']\n",
    "for word in words:\n",
    "    print(f\"Word: {word}, Lemma: {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac25767",
   "metadata": {},
   "source": [
    "Exercise 13\n",
    "\n",
    "Apply the Lemmatizer to the text file you opended in exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1636cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lemmatization:\n",
      "\n",
      "['Artificial', 'intelligence', 'transforming', 'world', '.', 'It', 'used', 'healthcare', ',', 'finance', ',', 'education', ',', 'many', 'field', '.', 'AI', 'technology', 'improving', 'efficiency', 'decision-making', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"After Lemmatization:\\n\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bb529",
   "metadata": {},
   "source": [
    "Exercise 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42ba49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "This is an example of sentence segmentation.\n",
      "It demonstrates how sentence segmentation works using NLTK.\n",
      "Hope you find it helpful.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text = \"Hello! This is an example of sentence segmentation. It demonstrates how sentence segmentation works using NLTK. Hope you find it helpful.\" \n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "for sentences in sentences:\n",
    "    print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed9b3a",
   "metadata": {},
   "source": [
    "Exercise 15: \n",
    "Using the previous examples develop a text summarisation script that would read\n",
    "a text file and summarise it into 3 sentences.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afde31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original Text from File:\n",
      "\n",
      "Artificial intelligence is transforming the world. \n",
      "It is used in healthcare, finance, education, and many other fields. \n",
      "AI technologies are improving efficiency and decision-making.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Read text from file\n",
    "with open(\"text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\" Original Text from File:\\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f981c9",
   "metadata": {},
   "source": [
    "A. Tokenize the sentences and words in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6681307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Artificial intelligence is transforming the world.', 'It is used in healthcare, finance, education, and many other fields.', 'AI technologies are improving efficiency and decision-making.']\n",
      "\n",
      "Words: ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', '.', 'it', 'is', 'used', 'in', 'healthcare', ',', 'finance', ',', 'education', ',', 'and', 'many', 'other', 'fields', '.', 'ai', 'technologies', 'are', 'improving', 'efficiency', 'and', 'decision-making', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(file_text)\n",
    "words = word_tokenize(file_text.lower())\n",
    "\n",
    "\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"\\nWords:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70eb507",
   "metadata": {},
   "source": [
    "B. Removes any stopwords from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afceca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['artificial', 'intelligence', 'transforming', 'world', 'used', 'healthcare', 'finance', 'education', 'many', 'fields', 'ai', 'technologies', 'improving', 'efficiency']\n"
     ]
    }
   ],
   "source": [
    "#  Removing stopwords \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cdcaf",
   "metadata": {},
   "source": [
    "C. Calculates the frequencies of each word, excluding the stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7f9c7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequencies: {'artificial': 1, 'intelligence': 1, 'transforming': 1, 'world': 1, 'used': 1, 'healthcare': 1, 'finance': 1, 'education': 1, 'many': 1, 'fields': 1, 'ai': 1, 'technologies': 1, 'improving': 1, 'efficiency': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "freq = defaultdict(int)\n",
    "for word in filtered_words:\n",
    "    freq[word] += 1\n",
    "\n",
    "print(\"Word Frequencies:\", dict(freq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde35ba",
   "metadata": {},
   "source": [
    "D. Calculates the score for each sentence based on the frequencies of the words it \n",
    "contains, considering only sentences with less than 30 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dc76a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Scores: {'Artificial intelligence is transforming the world.': 4, 'It is used in healthcare, finance, education, and many other fields.': 6, 'AI technologies are improving efficiency and decision-making.': 4}\n"
     ]
    }
   ],
   "source": [
    "sentence_scores = {}\n",
    "for sentence in sentences:\n",
    "    word_count = len(word_tokenize(sentence))\n",
    "    if word_count < 30:  # only short sentences\n",
    "        score = sum(freq[word.lower()] for word in word_tokenize(sentence) if word.lower() in freq)\n",
    "        sentence_scores[sentence] = score\n",
    "\n",
    "print(\"Sentence Scores:\", sentence_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d6591",
   "metadata": {},
   "source": [
    "E. Selects the top 2 sentences with the highest scores to generate a summary.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a430ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Sentences: ['It is used in healthcare, finance, education, and many other fields.', 'Artificial intelligence is transforming the world.']\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "top_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)\n",
    "print(\"Top Sentences:\", top_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbe3ff",
   "metadata": {},
   "source": [
    "F. Combines these top sentences to create the final summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2da1b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " It is used in healthcare, finance, education, and many other fields. Artificial intelligence is transforming the world.\n"
     ]
    }
   ],
   "source": [
    "summary = ' '.join(top_sentences)\n",
    "print(\"Summary:\\n\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
